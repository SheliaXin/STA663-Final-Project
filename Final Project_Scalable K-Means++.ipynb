{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> STA663 Final Project: Scalable K-means\n",
    "### <center> Xin Xu, Fu Wen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Abstract "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to its simplicity, the k-means algorithm is one of the most famous machine learning algorithms used to cluster data[1]. The main critical problem of this algorithm is that it might be blocked locally based on the initial random chosen centers. The k-means++ algorithm is developed to solve this problem, spreading out the initial centers with an updating non-uniform distribution. However, K-means++ has a limited applicability to large data sets due to its inherent sequential nature, which requires k passes through the whole data set to find the optimal initialization of centers. The K-means|| algorithm in the paper \"Scalable K-Means++\" is the parallel version of the k-means++[2] and an improvement. Instead of sampling a single point, it oversamples a couple of centers in each iteration and guarantees the performance at the same time. In this report, we firstly implemented the K-means|| algorithm in Python. Then, we parallelized the algorithm in Spark and applied it to the GAUSSMIXTURE dataset simulated as in [1] and the SPAM dataset from UC Irvine Machine Learning repository [3]. In the end, we compared the clustering cost and convergence speed with the k-means|| algorithm to the result of the k-means and the k-means++."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Background and Related Algorithm\n",
    "\n",
    "As one of the most popular clustering algorithms, the k-means algorithm has been widely used for the last half of the century[4]. The main idea is to randomly choose k centers, repeatedly assign each point to its nearest center and calculate the new centers by minimizing the sum of the squares of the distance in its neighborhood.\n",
    "\n",
    "The k-means algorithm has a critical problem of unreliable initialization[2]. This algorithm with an incorrect initialization cannot find a globally optimal solution but rests on a locally optimal solution. In addition, the running time before convergence is long. K-means++ algorithm avoids this problem by finding k better initial centers. It first samples one random point uniformly from the data, then sets the subsequent k-1 centers with probability proportional to its contribution to the overall error given the previous centers. In contrast to the k-means algorithm, k-means++ reduces the probability of picking several initial centers in one cluster. However, the sequence initialization process also limits its applicability to large data sets or data with large k since the whole algorithm is not scalable.\n",
    "\n",
    "Bahmani et al. constructed the scalable k-means++ algorithm (k-means|| algorithm) in their paper \"Scalable K-Means++\"[1]. The main idea is to sample more than one point (O(k)) in each round and repeat the process for fewer iterations (O(logn)). Then, the algorithm reclusters the O(klogn) points generated from the above process into k initial centers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Notation and the Algorithm\n",
    "\n",
    "Suppose $X = \\{x_1, \\dots, x_n \\}$ are d-dimentional points to be clustered and k is the number of cluster (a positer integer). \n",
    "\n",
    "For a subset $Y \\in X$, define the distance from a point $x$ to $Y$ as $d(x,Y) = min_{y \\in Y} \\| x-y\\|$, where $\\|x -y \\|$denote the Euclidean distance between $x$ and $y$, define the centroid of $Y$ as\n",
    "$$Centroid(Y) = \\frac{1}{|Y|} \\sum_{y \\in Y} y$$\n",
    "\n",
    "For a set of cluster centers $C = \\{ c_1, c_2 ,\\dots , c_k\\}$, define the _cost of $Y$_ with respect to $C$ as:\n",
    "\n",
    "$$ \\phi_Y(C) = \\sum_{y \\in Y} d(y,C)^2$$\n",
    "\n",
    "In k-means|| algorithm, it set an oversampling factor $l = \\Omega (k)$. $l>1$ is an integer.\n",
    "\n",
    "1. Sample a point uniformaly from X as the first center $C$\n",
    "2. Compute the cost of clustering based on this choice $\\phi_{X}(C)=\\psi$\n",
    "3. for $O(log\\psi)$ times repeat:  \n",
    "a. Independently sample $l$ points with probability $p_{x}=\\frac{l\\cdot d^{2}(x,C)}{\\phi_{X}(C)}$ as $C'$\n",
    "b. $C=C\\cup C'$\n",
    "4. For each point $x \\in C$, compute $w_{x}$ as the number of points in $X$ closer to x than other point in $C$\n",
    "5. Get k clusters from reclustering those weighted points in $C$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "### Simulate Data \n",
    "\n",
    "To generate the dataset GAUSSMIXTURE, we sampled k=5 centers from a 15-dimensional spherical Gaussian distribution with mean at the origin and variance $R\\in \\{1,10,100 \\}$ and then added points from Gaussian distributions with unit variance around each center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Simulate data\n",
    "k = 20\n",
    "n = 10000\n",
    "d = 15\n",
    "\n",
    "## simulate k centers from 15-dimensional spherical Gaussian distribution \n",
    "mean = np.hstack(np.zeros((d,1)))\n",
    "cov = np.diag(np.array([1,10,100]*5))\n",
    "centers = np.random.multivariate_normal(mean, cov, k)\n",
    "\n",
    "## Simulate n data\n",
    "for i in range(k):\n",
    "    mean = centers[i]\n",
    "    if i == 0:\n",
    "        data = np.random.multivariate_normal(mean, np.diag(np.ones(d)) , int(n/k+n%k))\n",
    "    else:\n",
    "        data = np.append(data, np.random.multivariate_normal(mean, np.diag(np.ones(d)) , int(n/k)), axis = 0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High performance computing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application and comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[1] Wu, Xindong, et al. \"Top 10 algorithms in data mining.\" Knowledge and information systems 14.1 (2008): 1-37.\n",
    "\n",
    "[2] Bahmani, Bahman, et al. \"Scalable k-means++.\" Proceedings of the VLDB Endowment 5.7 (2012): 622-633\n",
    "\n",
    "[3] Lichman, M. (2013). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
